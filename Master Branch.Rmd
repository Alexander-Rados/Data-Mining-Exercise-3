---
title: "Exercise 3"
author: "Alex Rados, Akhil Jonnalagadda, and Kenny Kato"
output: 
  github_document:
    toc: True
---
# Predictive Model Building
```{r include=FALSE}
library(tidyverse)
library(lubridate)
library(randomForest)
library(pdp)
library(doMC)
library(foreach)
library(mosaic)
options('mosaic:parallelMessage' = FALSE)
greenbuildings = read.csv("~/Documents/Data Mining/data/greenbuildings.csv", header=TRUE)
``` 
## Overview  
The goal at hand is producing the best model to predict rental income per square foot of commercial rent buildings across the United States. The main variable we are interested in is their green rating, whether the building has been awarded LEED or EnergyStar certification (or both) as a green building, and how that affects the rental price. We will thus build a predictive model for price using the variables provided and evaluate the effect of having a green rating or not.

## Data
We will be looking at the green buildings dataset to assess how the rent price (in dollars per square foot per calendar year) of commercial rent buildings across the United States is effected by a variety of variables including size (total square footage of available renting space), number of floors, and their green rating, to list a few.

Using these variables, we will fit a random forest model, attempting to get the most accurate prediction of rent prices based on the variables provided. The accuracy of the model will be measured in the form of the root mean-squared error (RMSE). Once we fit said model, we will use it to quantify the average change in rental income per square foot associated with green certification, holding other features of the building constant.

```{r echo=FALSE, warning=FALSE, cache=TRUE}
# Train/Test splits
# training and testing sets
n = nrow(greenbuildings)
n_train = floor(0.8*n)
n_test = n - n_train
```

In order to generate the most accurate predictions, however, we can't judge it based on past data that we have. Thus, we randomly sampled 80% of the data at hand and used that as a training set on which we built our model while using the other 20% as a test set on which we tested our model to gather a sufficient RMSE. This then allows us to measure the out-of-sample performance.

## Model
I decided on using a random forest model as it would give accurate and legitimate results while not needing the specifics that boosting requires. I initially began by setting a random forest model to variables that I assumed would have a significant effect on rent price. 

I began to whittle down the components of the model, getting rid of or adjusting the variables that had the smallest effects on the SSE when being included depending on the variable imporance plot.
```{r echo=FALSE, warning=FALSE, cache=TRUE}
out_forest1 = do(200)*{
  train_cases = sample.int(n, size=n_train, replace=FALSE)

  greenbuildings_train = greenbuildings[train_cases,]
  greenbuildings_test = greenbuildings[-train_cases,]
  
  y_all = greenbuildings$Rent
  
  # stories, elect_costs, size, gas_costs, age all very important
  x_all = model.matrix(~age + class_a + class_b + green_rating + amenities + 
                         stories + renovated + total_dd_07 + 
                         Precipitation + Electricity_Costs + Gas_Costs + 
                         size, data=greenbuildings)
  
  y_train = y_all[train_cases]
  x_train = x_all[train_cases,]
  
  y_test = y_all[-train_cases]
  x_test = x_all[-train_cases,]
  
  # Fit the model with default parameter settings
  forest1 = randomForest(x=x_train, y=y_train, xtest=x_test)
  yhat_test = (forest1$test)$predicted

  # RMSE
  (yhat_test - y_test)^2 %>% mean %>% sqrt
}

# a variable importance plot: how much SSE decreases from including each var
varImpPlot(forest1)
```

I finally ended up settling on the following model that resulted in the lowest RMSE when attempting to predict the out-of-sample rent prices. This was after ensuring that the random variation that comes with a train/test split along with the model itself is taken care of and is guaranteeing that we are choosing the model that is consistently the best predictor.

$$y = Agex_{1} + Class_ax_{2} + Class_bx_{3} + Green Ratingx_{4} + Amenitiesx_{5} + Sizex_{6} + Storiesx_{7} + Renovatedx_{8} + Precipitationx_{9} + Gas Costsx_{10} + Electricity Costsx_{11} + Total Degree Daysx_{12}$$

```{r echo=FALSE, warning=FALSE, cache=TRUE}
# RMSE
mean(out_forest1$result)
```

From this model, the average change in absolute terms in rental income per square foot associated with whether one has a green certification or not is as follows:

```{r echo=FALSE, warning=FALSE, cache=TRUE}
greenbuildings_rf = randomForest(Rent ~ age + class_a + class_b + green_rating + 
                                   stories + renovated + total_dd_07 + 
                                   Precipitation + Electricity_Costs + Gas_Costs + 
                                   size + amenities, data=greenbuildings)
p1 = pdp::partial(greenbuildings_rf, pred.var = 'green_rating') 
p1
```
## Conclusion
Through using a random forest model, we found an accurate way of predicting rental income per square foot with a variety of variables. While many made significant impacts on the dependent variable at hand, the most important ones were electricity costs in the building's geographic region and size of the rental space. The least important components were indeed whether the building was green certified by either LEED or EnergyStar and the amenities that were included. 

Thus, if a leasing company is looking to make the most out of the factors that go into constructing and leasing a building, we would recommend focusing on choosing the right region surrounding the project and the size of the available space for rent.

# What Causes What?

1. Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city.)

There is legitimacy to either argument that higher crime rates force a leader to increase the size of the police force (high crime rates for large police force) or a larger police force leads to lower crime rates (low crime rates for large police force), giving opposing results to how the amount of police affects crime rates. 

We also are not necessarily asking a causal question here but rather a correlation question, so a regression of crime on police wouldn't answer "does increasing the amount of cops lead to lower crime" because crime rates affect the amount of police deployed (given one lives in a reasonable neighborhood) yet the amount of police would also affect crime rates. Because police is an endogenous variable, we would have to bring in an instrumental variable that is exogenous and is related to the size of the police force while not directly related to street level crime rates. Enter, terrorism alert level.

2. How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2”, from the researchers' paper.

You can't effectively estimate street crime rates just by looking at level of police force because of what we described earliers. Thus, researchers wanted to find an example where there's a lot of police for reasons unrelated to crime, which ended up being the terrorism alert system. This represents an instrumental variable, a variable that is related to amount of police used but is not related at all with street crime. 

When the terror alert level goes to orange, extra police are used in Washington D.C. to protect against possible terror attacks. This shouldn't be related at all to street crime, so then they could evaluate those days when there’s extra police to see what happens to street crime and not worry about street crime having an effect on the amount of police deployed. Street crime does indeed go down in this case and is significant at the 5% level, allowing us to reject the null that the number of police doesn't affect the crime rate.

3. Why did they have to control for Metro ridership? What was that trying to capture?

They questioned whether it was possible that tourists were less likely to be out when there is a terror alert level that is orange, which would remove one of the two requirements for an instrumental variable that it isn't correlated with the outcome, crime rate. To combat this, they controlled for metro ridership, thus including the possibility that the street crime rate didn’t drop just because there were less people outside (or, in the case of the original question proposed on the podcast, that the criminals weren’t staying at home because of the threat of terrorsim). 

4. Below I am showing you "Table 4" from the researchers' paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?

The model being estimated is a regression of crime rates in Washington D.C. on high alert (whether there was a terror alert level of orange which provides an unusual increase in the amount of police present) interacted with the first district of Washington D.C. and high alert interacted with all the other districts while still controlling for Metro ridership. 

We are seeing that there is a difference in the high alert police level effects on crime rates between District 1 and every other district. When there is an unusual influx of police to District 1 because of a high terror level, it decreases crime rates more drastically than when there is an increase in police in every other district (for District 1 it would be a significant decrease at the 1% level while for the other districts it wouldn’t be significant at all). This provides some evidence that maybe an increase in police in District 1 provides a decrease in street crime while for every other district we can’t reject the null that the crime rate does not change with a change in amount of police. 

# Clustering and PCA
```{R include=FALSE, cache=TRUE}
library(ggplot2)
library(foreach)
library(mosaic)
library(dplyr)
wine = read.csv("~/Documents/Data Mining/data/wine.csv")
wine <- mutate(wine, id = rownames(wine))
wine1= wine
wine$color <- ifelse(wine$color == "red", 1, 0)
Winecolor=select(wine,-c(12))
WineQ=select(wine,-c(13))
# Center and scale the data
X = Winecolor[,(1:11)]
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
#pick K
wss <- (nrow(Winecolor)-1)*sum(apply(Winecolor,2,var))
#finding within sum of squares
for (i in 2:15) wss[i] <- sum(kmeans(Winecolor,centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares",col="mediumseagreen",pch=12)
# Run k-means with 3 clusters and 25 starts
clust1 = kmeans(X, 3, nstart=1000)
# What are the clusters?
clust1$center  # not super helpful
clust1$center[1,]*sigma + mu
clust1$center[2,]*sigma + mu
clust1$centers[3,]*sigma + mu
# Which wines are in which clusters?
one= which(clust1$cluster == 1)
two=which(clust1$cluster == 2)
three=which(clust1$cluster == 3)
###### link variables to wine set
first <- Winecolor[c(one), ]
second = Winecolor[c(two),]
third = Winecolor[c(three),]
## id the clusters
first= mutate(first, id =rownames(first))
second= mutate(second, id =rownames(second))
third= mutate(third, id =rownames(third))
#### map to white or red 
first$color= mean(first$color)
first$color <- ifelse(first$color >= .5, "red", "white")
second$color= mean(second$color)
second$color <- ifelse(second$color >= .5, "red", "white")
third$color= mean(third$color)
third$color <- ifelse(third$color >= .5, "red", "white")
### predicted list of colors
predict1= rbind(first,second,third)
predict1[order(predict1$id),]
predict2=predict1
predict= predict1
### split and list by ID
red <- subset(predict1, color =="red")
white = subset(predict2, color =="white")
### graphically show 
```

This problem required us first to pick an appropriate clustering method and run both that method and PCA. From this we had to choose the optimal method in identifying colors red and white. Logically, I used K Means clustering to separate these wines. This data set had a handful of variables that were used to measure each sample and it mapped to quality and color. The goal of K clustering is to average over these variables so that the average of each samples' characteristics would link to the nearest similar cluster. 

In my analysis, I had to figure out what K to use. I created an elbow plot, a graph of the mean sum of squares where the optimal K would be its tangent point. I found the ideal K would be 3 for this analysis.

```{R echo=FALSE, cache=TRUE, fig.width=5, fig.height=5}
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares",col="mediumseagreen",main="K plot",pch=12)
```

After finding the optimal K, I used it to run K Means and create 3 clusters. To run this analysis I made colors binary, red=1 and white=0. I linked the appropriate clusters to red or white. If the value of the color column was greater than .5 then the cluster would be classified as a red wine, and if not, it would be classified as a white. From this we had our predictions of red and white wines, as shown in the following graphs.

```{R echo=FALSE, cache=TRUE}
print(
  (ggplot(predict, aes(x = sulphates, y = total.sulfur.dioxide, colour = color)) + geom_point() + labs(title="Sulfar vs Sulfates")))
 print( ggplot(predict, aes(x = citric.acid, y = chlorides, colour = color)) + geom_point() + labs(title="Citric Acid vs Chlorides"))
  
 print (ggplot(predict, aes(x = fixed.acidity, y = residual.sugar, colour = color)) + geom_point() + labs(title= "Acidity vs Sugar"))
```

From these three graphs we see that there is grouping of reds and whites. The red wines have higher levels of sulfur and a smaller range of sulfate levels. The second graph shows us that citric acids are relatively spread the same but red wines have hiring chloride counts than white. Our third graph shows us that red wines are not as sweet as white wine and tend to have a higher spread of acidity. 

We also see an important factor to consider is that there is overlap of red and white wines at times. This is due to the fact that the clustering is not perfect and we thus face some level of error as wines are similar in the chemical breakdown 
Now we should compare to the PCA analysis to separate reds and whites. I scaled the data set and sorted the data and ran PCA. The summary showed that the first two components explained most of the variation of the data. From this, the data was plotted and mapped to show reds and whites. We see that the difference is more distinct when separating the two groups. We see that PCA does a better job of decipering between red and white wines with less overlap of the data, making it a better technique.

```{R include=FALSE, cache=TRUE}
#PCA
library("cluster")
rm(wine1)
wine1=wine
wine1 = group_by(wine1,quality)
colorlist= wine1$color
wine1= select(wine1,-(14))
wine1= select(wine1,-13)
wine1= scale(wine1)
pc= prcomp(wine1, scores=TRUE, cor= TRUE)
summary(pc)
plot(pc)
plot(pc,type = "l", main ="Scree plot for PCA")
biplot(pc)
#For PC 1
PC_1 <- pc$rotation[,1]
PC_1 <- abs(PC_1)
PC_1<- names(sort(PC_1,decreasing = T))
#For PC 2
PC_2 <- pc$rotation[,2]
PC_2 <- abs(PC_2)
PC_2<- names(sort(PC_2,decreasing = T))
pc$rotation[PC_1,1]
pc$rotation[PC_2,2]
pca.color <- data.frame(wine1, pc$x[,1:2])
pca.color = cbind(pca.color,colorlist)
pca.color$colorlist <- ifelse(pca.color$colorlist == 1, "red", "white")
```

```{R echo=FALSE,fig.width=5, fig.height=5}
print(ggplot(pca.color,aes(x=PC1,y=PC2,colour= colorlist)) + geom_point() + labs(title="Red vs White")
)
```

## Wine Quality Ranking
Part two of this assignment was to find a way to rank the quality of wines. I went with a PCA because it was more accurate on this data set and it works better when there are higher levels of correlation like in our wine set. 

I ran a similar method to the PCA earlier for the wine quality problem. The goal was to split the quality of wines into seven individual groups. From our graph we see there is some slight differences between the groups. The left chunk is slightly lighter than the right chunk. This shows there is some light separation of quality. However as a whole we can't find definite segments of quality.

The reason that this could be that there is a high level of correlation between all the variables regardless of quality. By this I mean there is less differentiation of wine quality from the chemical make up. 

```{R include=FALSE, cache=TRUE}
rm(wine2)
wine2=wine
wine2 = group_by(wine2,color)
qualitylist= wine2$quality
wine2= select(wine2,-(14))
wine2= select(wine2,-12)
wine2= scale(wine2)
pca= prcomp(wine2, scores=TRUE, cor= TRUE)
summary(pca)
plot(pca)
plot(pca,type = "l", main ="Scree plot for PCA")
biplot(pca)
#For PC 1
PCA1 <- pca$rotation[,1]
PCA1 <- abs(PCA1)
PCA1<- names(sort(PCA1,decreasing = T))
#For PC 2
PCA2 <- pca$rotation[,2]
PCA2 <- abs(PCA2)
PCA2<- names(sort(PCA2,decreasing = T))
pca$rotation[PCA1,1]
pca$rotation[PCA2,2]
pcaQ <- data.frame(wine2, pca$x[,1:2])
pcaKMeans = pcaQ 
pcaQ = cbind(pcaQ,qualitylist)
```

```{R echo=FALSE,fig.width=5, fig.height=5, cache=TRUE}
print(ggplot(pcaQ,aes(x=PC1,y=PC2, colour= qualitylist)) + geom_point() + labs(title="Quality Spread")
)
```

After our failed PCA for quality I tried out K means approach to see if there was an alternative approach. I ran a parallel method as the wine color clustering method to cluster for quality. I ran into a similar issue where I could not separate for each of the qualities . In the density graphs we see that the majority of the wines grouped around 5 and 6. This supports that the individual wine qualities were not identified. We also see that the majority of the clusters were showing clusters that averaged around 5 or 6. This supports the idea that the variables were too highly correlated to help us segment quality as the clusters were just similar to average quality of the data set.

```{R include=FALSE, cache=TRUE}
#### RUN SIMILAR PROCESS FOR QUALITY
  
  
  ####
# Center and scale the data
Y = WineQ[,(1:11)]
Y = scale(Y, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
m = attr(Y,"scaled:center")
s = attr(Y,"scaled:scale")
#pick K
wss <- (nrow(WineQ)-1)*sum(apply(WineQ,2,var))
#finding within sum of squares
for (i in 2:15) wss[i] <- sum(kmeans(WineQ,centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares",col="mediumseagreen",pch=12)
# Run k-means with 4 clusters and 25 starts
# Run k-means with 4clusters and 1000 starts
clust2 = kmeans(Y, 4, nstart=1000)
# What are the clusters?
clust2$center  # not super helpful
clust2$center[1,]*sigma + mu
clust2$center[2,]*sigma + mu
clust2$centers[3,]*sigma + mu
clust2$center[4,]*sigma + mu
# Which wines are in which clusters?
on= which(clust2$cluster == 1)
tw=which(clust2$cluster == 2)
th=which(clust2$cluster == 3)
f=which(clust2$cluster == 4)
###### link variables to wine set
fi <- WineQ[c(on), ]
s = WineQ[c(tw),]
t = WineQ[c(th),]
fo  = WineQ[c(f),]
## id the clusters
fi= mutate(fi, id =rownames(fi))
s= mutate(s, id =rownames(s))
t= mutate(t, id =rownames(t))
fo= mutate(fo, id =rownames(fo))
```
```{R echo=FALSE, cache=TRUE,fig.width=3.5, fig.height=4}
### density
print(ggplot(fi, aes(x=quality)) + 
  geom_density()+ labs(title="Cluster 1"))
print(ggplot(s, aes(x=quality)) + 
  geom_density() + labs(title="Cluster 2"))
print(ggplot(t, aes(x=quality)) + 
  geom_density() + labs(title="Cluster 3"))
print(ggplot(fo, aes(x=quality)) + 
  geom_density() + labs(title="Cluster 4"))
```

## Conclusion
In summary of analysis, both K means clustering and PCA were able to show us separate segments of red and white. When running a similar process for wine quality we found that it was not possible to separate each wine on the quality rating. The assumption is that the wines are all very similar in their breakdown regardless of the quality group they fall in.

# Market Segmentation

**Background:**
Defined market segments are fundamental for guiding a marketing strategy and as such, we valued clarity, ease of communication, and relevance throughout this project.  Market segmentation, in our view, is not just a categorization scheme, but a basis for action.

While we tried agglomerative hierarchical clustering (since we suspected that tags like “chatter” might be an umbrella with several distinct/relevant sub-characteristics) and principal components analysis (PCA), in order to distill any correlations, we thought that K-means++ conveyed the clearest insights.  By use of K-means++, we are able to discern and characterize 6 distinct market segments that may help inform the marketing strategy of NutrientH20.

```{r include=FALSE, cache=TRUE}
library(LICORS)
library(mosaic)
library(cluster)

social_marketing <- read.csv("~/Documents/Data Mining/data/social_marketing.csv")

# delete the untrustworthy records

social_marketing <- social_marketing[!(social_marketing$adult > 0 & social_marketing$spam > 0),]
social_marketing$total <- rowSums(social_marketing[-1])  # totaling number of tags
social_marketing$pctadult <- (social_marketing$adult/social_marketing$total)  # excessive amount of adult posts
social_marketing <- social_marketing[-(which(social_marketing$pctadult >= .25)),]

# delete irrelevant features

social_marketing <- social_marketing[,-c(36,39)]  # removed spam and pctadult columns
```

## Pre-Processing
To prune the dataset, we:

* Deleted records that carried at least one “adult” and one “spam” label, judging their credibility to be in question while not significantly slashing the size of the dataset
* Deleted records whose labels were more than 25% “adult,” for both credibility and usefulness concerns (our thinking being that excessively adult-oriented users are unlikely to inform any useful marketing strategy for NutrientH20)
* Deleted the “spam” feature, since only three records left had a single spam label

Overall, we think these steps made a more trustworthy and useful dataset.

```{r include=FALSE, cache=TRUE}
# center and scale

sm_scaled <- scale(social_marketing[,-c(1,37)], center = TRUE, scale = TRUE)
mu = attr(sm_scaled, "scaled:center")
sigma = attr(sm_scaled, "scaled:scale")


# K-means++

set.seed(6)
kpp = kmeanspp(sm_scaled, 6, nstart = 25)

c1 <- data.frame(social_marketing[(which(kpp$cluster == 1)),])
c2 <- data.frame(social_marketing[(which(kpp$cluster == 2)),])
c3 <- data.frame(social_marketing[(which(kpp$cluster == 3)),])
c4 <- data.frame(social_marketing[(which(kpp$cluster == 4)),])
c5 <- data.frame(social_marketing[(which(kpp$cluster == 5)),])
c6 <- data.frame(social_marketing[(which(kpp$cluster == 6)),])

sum1 <- data.frame(cbind(colSums(c1[,-c(1,37)]),
                         kpp$center[1,]*sigma + mu,
                         colSums(c2[,-c(1,37)]),
                         kpp$center[2,]*sigma + mu,
                         colSums(c3[,-c(1,37)]),
                         kpp$center[3,]*sigma + mu,
                         colSums(c4[,-c(1,37)]),
                         kpp$center[4,]*sigma + mu,
                         colSums(c5[,-c(1,37)]),
                         kpp$center[5,]*sigma + mu,
                         colSums(c6[,-c(1,37)]),
                         kpp$center[6,]*sigma + mu))

colnames(sum1)[c(1,3,5,7,9,11)] <- "Label Count"
colnames(sum1)[c(2,4,6,8,10,12)] <- "Center Value"

t1 <- head(sum1[(order(sum1[,1], decreasing = TRUE)), 1:2, drop = FALSE], n=8)
t2 <- head(sum1[(order(sum1[,3], decreasing = TRUE)), 3:4, drop = FALSE], n=8)
t3 <- head(sum1[(order(sum1[,5], decreasing = TRUE)), 5:6, drop = FALSE], n=8)
t4 <- head(sum1[(order(sum1[,7], decreasing = TRUE)), 7:8, drop = FALSE], n=8)
t5 <- head(sum1[(order(sum1[,9], decreasing = TRUE)), 9:10, drop = FALSE], n=8)
t6 <- head(sum1[(order(sum1[,11], decreasing = TRUE)), 11:12, drop = FALSE], n=8)
```

## Market Segments (K-Means++)

**Choosing K:**
Our first task was to choose a reasonable number of clusters.  In pursuit of an optimal number, we employed an elbow plot, the CH-Index, and the gap statistic, all to no avail, so we decided to just pick one.  After some test runs, we settled on 6 clusters that were most identifiable.

Each cluster is listed below, ordered by size from smallest to largest, with their eight most common labels and a short description (it should be noted that "chatter" and "photo-sharing" are in each cluster's top 8):

#### The Youthful:
Number of users:
```{r echo=FALSE}
nrow(c3)
```
Top 8 labels:
```{r echo=FALSE}
t3 # young
```
This cluster - the smallest - looks like a young group: concerned with college, with time to spend on gaming, and energy to spend on playing sports.  At risk of sounding old-fashioned, these traits also appear to be those traditionally associated with males.

#### The Aesthetic:
Number of users:
```{r echo=FALSE}
nrow(c1)
```
Top 8 labels:
```{r echo=FALSE}
t1 # aesthetic
```
This group must be the best-looking: it has very photogenic interests, and exploits as much, as evidenced by the high ranking of "photo-sharing."  Their particular affinity for cooking could explain their interest in NutrientH20.  Again risking sounding old-fashioned, these appear to be traditionally female interests.

#### The Worldly:
Number of users:
```{r echo=FALSE}
nrow(c6)
```
Top 8 labels:
```{r echo=FALSE}
t6 # worldly
```
Dominated by politics, travel, and news, this group is probably full of subscribers to the New York Times and #wanderlust hashtags.

#### The Domestic:
Number of users:
```{r echo=FALSE}
nrow(c5)
```
Top 8 labels:
```{r echo=FALSE}
t5 # domestic/conservative
```
This seems to be a fairly conservative group of people, focused much more on close-to-home issues.  Hilariously, as much as they post about family, parenting, and religion, their sports team is second-to-none.

#### The Healthy:
Number of users:
```{r echo=FALSE}
nrow(c4)
```
Top 8 labels:
```{r echo=FALSE}
t4 # healthy
```
Not surprisingly, followers of NutrientH20 tend to post about health/nutrition, fitness, cooking, and the great outdoors.  This is the largest cluster of users that don't belong to...

#### Everybody Else:
Number of users:
```{r echo=FALSE}
nrow(c2)
```
Top 8 labels:
```{r echo=FALSE}
t2 # everyone else
```
Finally, this cluster - by far the largest - looks like somewhat of a hodgepodge of characteristics, dominated by generic traits like "chatter" and "photo sharing."  It admittedly seems like a cop-out to call them "everybody else" when we're trying to identify characteristics; however, this cluster is distinguished by one thing in particular:

## Frequent vs. Infrequent Users
While we don’t have information on how many posts each user posted over the sample week, we can use the number of labels associated with them as a proxy, assuming the number of labels per individual post is fairly invariant across users.  Here is the distribution of labels:

```{r include=FALSE, cache=TRUE}
labelhist <- ggplot(data = social_marketing) +
  geom_bar(mapping = aes(total)) +
  ggtitle("Label Count") +
  labs(x = "Number of Labels", y = "Number of Users") +
  theme(plot.title = element_text(hjust = 0.5, size = 22))
```
```{r echo=FALSE}
labelhist
```

For each cluster, we divided the total number of labels by the number of users to obtain average labels per user.  All 5 of our minority clusters average around 60 labels per user – on the higher end of the above distribution.  The sixth and largest cluster averages around 28, which is where we see the largest mass of users in the distribution.  We interpret that to mean that frequent posters are more likely to convey tendencies than less frequent posters (and are more engaged with Twitter itself). K-means++ diligently sorted out the trends it could confidently find from frequent posters and apparently sorted the less frequent posters into their own cluster.

## Conclusion
While it might be initially frustrating that our largest cluster seems to lack actionable distinctions, we think that patterns shown by users who display a high engagement with Twitter are highly valuable, and possibly represent latent behaviors of our less frequent posters.  By targeting the revealed patterns of minority, high-frequency users - whether they are young, fashionable, worldly, domestic, or health-conscious - NutrientH20 may yet increase the engagement of Everybody Else.
